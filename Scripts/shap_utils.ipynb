{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.model_selection import KFold\n",
    "import xgboost as xgb \n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold, train_test_split\n",
    "from xgboost import XGBClassifier, cv\n",
    "#from shaphypetune import BoostSearch, BoostRFE, BoostRFA, BoostBoruta\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm\n",
    "import shap\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, f1_score\n",
    "from scipy.stats import pearsonr\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "import seaborn as sn\n",
    "# Run classifier with cross-validation and plot ROC curves\n",
    "import scipy\n",
    "import shap\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assign_colour(correlation):\n",
    "    if correlation > 0:\n",
    "        return \"#ffa09b\"  # red\n",
    "    else:\n",
    "        #return \"#9eccb7\"  # green\n",
    "        return \"#0047ab\"  # blue\n",
    "\n",
    "\n",
    "def assign_thickness(correlation, benchmark_thickness=4, scaling_factor=1000):\n",
    "    return benchmark_thickness * abs(correlation) ** scaling_factor\n",
    "\n",
    "\n",
    "def assign_node_size(degree, scaling_factor=35):\n",
    "    return degree * scaling_factor\n",
    "\n",
    "# helper method for getting legend\n",
    "def get_legend(sizes, num=10):\n",
    "    fig, ax = plt.subplots()\n",
    "    sc = ax.scatter([0]*len(sizes), [0]*len(sizes), s=sizes, color='blue')\n",
    "    store = sc.legend_elements(\"sizes\", num=num)\n",
    "    fig.clf()\n",
    "    return store\n",
    "\n",
    "def assign_node_clustering(Gx):\n",
    "    cluster_ids = np.unique(list(nx.clustering(Gx).values()))\n",
    "    print(cluster_ids)\n",
    "    n=len(cluster_ids) # number of colors you want to get\n",
    "    cmap = plt.cm.get_cmap('rainbow', n) \n",
    "    my_map=dict(zip(cluster_ids, [cmap(i) for i in range(n)]))\n",
    "    colors=[]\n",
    "    for node, cluster in nx.clustering(Gx).items():\n",
    "        colors.append(my_map[cluster])\n",
    "    return colors\n",
    "\n",
    "def assign_node_colors(Gx):\n",
    "    node_cluster={}\n",
    "    cluster_ids=[]\n",
    "    for cluster_id, nodes in enumerate(nx.connected_components(Gx)):\n",
    "        cluster_ids.append(cluster_id)\n",
    "        for node in nodes:\n",
    "            node_cluster[node]=cluster_id\n",
    "    cmap = plt.cm.get_cmap('tab20', len(cluster_ids)) \n",
    "    my_map=dict(zip(cluster_ids, [cmap(i) for i in range(len(cluster_ids))]))    \n",
    "    colors=[]\n",
    "    for node in Gx.nodes():\n",
    "        colors.append(my_map[node_cluster[node]])\n",
    "        \n",
    "    return colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shap(model, X, original_df_labels, target_label, model_name, link=shap.links.logit, multiclass_index=None):\n",
    "    \n",
    "    npermutations =  max(500, (2*len(original_df_labels)+1))\n",
    "    if npermutations > 500:\n",
    "        explainer = shap.explainers.Permutation(model, X, feature_names=original_df_labels, link=link, max_evals = npermutations)\n",
    "    else:\n",
    "        explainer = shap.Explainer(model, X, feature_names=original_df_labels, link=link)\n",
    "    #calculate feature importances\n",
    "    #shap_values = explainer(X)\n",
    "    shap_values = explainer(X)\n",
    "    #samples, features, outcomes\n",
    "    print(\"SHAP shape\", shap_values.values.shape)\n",
    "    if multiclass_index != None:\n",
    "        shap_values.values =  np.squeeze(shap_values.values[:,:,multiclass_index])\n",
    "        print(\"SHAP shape after extracting class\", multiclass_index , \":\", shap_values.values.shape)\n",
    "\n",
    "    print(len(shap_values.values.shape))\n",
    "    if len(shap_values.values.shape) > 2:\n",
    "        shap_values_true = np.squeeze(shap_values.values[:,:,1::2])\n",
    "    else:\n",
    "        shap_values_true = shap_values.values\n",
    "        \n",
    "    print(\"shape1\", shap_values.shape)\n",
    "    #convert to absolute values\n",
    "    shap_values_abs = np.abs(shap_values_true)\n",
    "    #extract total importance of features\n",
    "    shap_values_abs_sum = shap_values_abs.sum(0)\n",
    "    print(\"shape2\", shap_values_abs_sum.shape)\n",
    "    #return indexes of each feature sorted by their impact\n",
    "    shap_values_abs_sum_argsort = shap_values_abs_sum.argsort()\n",
    "    \n",
    "    shap_v = pd.DataFrame(shap_values_true)\n",
    "    shap_v.columns = original_df_labels\n",
    "    df_v = pd.DataFrame(X, columns=original_df_labels)\n",
    "    \n",
    "    # Determine the correlation in order to plot with different colors\n",
    "    corr_list = list()\n",
    "    for i in original_df_labels:\n",
    "        b = np.corrcoef(shap_v[i],df_v[i])[1][0]\n",
    "        corr_list.append(b)\n",
    "    corr_df = pd.concat([pd.Series(original_df_labels),pd.Series(corr_list)],axis=1).fillna(0)\n",
    "    # Make a data frame. Column 1 is the feature, and Column 2 is the correlation coefficient\n",
    "    corr_df.columns  = ['Variable','Corr']\n",
    "    corr_df['Sign'] = np.where(corr_df['Corr']>0,'red','blue')\n",
    "    \n",
    "    # Plot it\n",
    "    shap_abs = np.abs(shap_v)\n",
    "    k=pd.DataFrame(shap_abs.mean()).reset_index()\n",
    "    k.columns = ['Variable','SHAP_abs']\n",
    "    k2 = k.merge(corr_df,left_on = 'Variable',right_on='Variable',how='inner')\n",
    "    \n",
    "    k2_out_prefix = 'abs_SHAP_feature_importance_'+target_label+'_'+model_name.replace(' ','_')\n",
    "    k2.to_csv(k2_out_prefix+'.csv', index=False)\n",
    "    \n",
    "    k2 = k2.sort_values(by='SHAP_abs',ascending = True)\n",
    "    k2 = k2.iloc[-50:]\n",
    "    colorlist = k2['Sign']\n",
    "    ax = k2.plot.barh(x='Variable',y='SHAP_abs',color = colorlist, figsize=(8,12),legend=False)\n",
    "    ax.set_xlabel(\"SHAP Value (Red = Positive Impact)\")\n",
    "    ax.set_title(target_label)\n",
    "        \n",
    "    out_prefix = 'SHAP_feature_importance_'+target_label+'_'+model_name.replace(' ','_')\n",
    "    plot_path=out_prefix+'_bar.pdf'\n",
    "    plt.savefig(plot_path, dpi=400, bbox_inches = \"tight\")\n",
    "    \n",
    "    fig = plt.figure(figsize =(8, 12))\n",
    "    sorted_indexes = shap_values_abs_sum_argsort[::-1]\n",
    "    print(shap_values.values.shape, type(shap_values.values))\n",
    "    mydf = pd.DataFrame(shap_values_true, columns=original_df_labels)\n",
    "    mydf.to_csv(out_prefix+'.csv', index=False)\n",
    "    \n",
    "    if len(shap_values.values.shape) > 2:\n",
    "        shap.plots.beeswarm(shap_values[:,:,1], max_display=50, plot_size=[8,12], order=sorted_indexes, show=False, color_bar=False)\n",
    "    else:\n",
    "        shap.plots.beeswarm(shap_values[:,:], max_display=50, plot_size=[8,12], order=sorted_indexes, show=False, color_bar=False)\n",
    "    \n",
    "    clb = plt.colorbar(shrink=0.33)\n",
    "    #clb.ax.set_title(ylabel='Feature value',fontsize=12)\n",
    "    clb.set_label(\"Feature value\")\n",
    "    \n",
    "    plt.title(target_label)\n",
    "    plot_path=out_prefix+'_beeswarm.pdf'\n",
    "    plt.savefig(plot_path, dpi=400, bbox_inches = \"tight\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return explainer\n",
    "\n",
    "def get_shap_interactions(model, X, original_df_labels, target_label, model_name, X_train = [], pert = 'tree_path_dependent', scaling_factor=8000, smoothing_factor=2, cor_threshold=0.80):\n",
    "    if len(X_train) == 0:\n",
    "        explainer = shap.Explainer(model, feature_names=original_df_labels)\n",
    "    else:\n",
    "        explainer = shap.Explainer(model, masker=X_train, feature_names=original_df_labels)\n",
    "    #explainer = shap.Explainer(model, X, feature_names=original_df_labels, feature_perturbation=pert)\n",
    " \n",
    "    shap_interaction = explainer.shap_interaction_values(X)\n",
    "    print('max', shap_interaction.max(), 'min', shap_interaction.min(), 'shape', shap_interaction.shape)\n",
    "\n",
    "    #shape is (6, 58, 42, 42)\n",
    "    #why 6? because:\n",
    "    #class 1 = 0, class 1 = 1\n",
    "    #class 2 = 0, class 2 = 1\n",
    "    #class 3 = 0, class 3 = 1\n",
    "    #tested with: np.asarray(classifiers['Random Forest'].predict_proba(X)).T[1]\n",
    "    #ci=0\n",
    "    #for c in range(1,np.asarray(shap_interaction).shape[0],2):\n",
    "    #    ci=ci+1\n",
    "    interaction_df = pd.DataFrame(shap_interaction.mean(0),\n",
    "                                  index=original_df_labels,\n",
    "                                  columns=original_df_labels)\n",
    "    print(np.asarray(shap_interaction).shape)\n",
    "    #print(shap_interaction[2][0][0])\n",
    "    #print(shap_interaction[3][0][0])\n",
    "\n",
    "    ######get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "    # Create a mask\n",
    "    mask = np.triu(np.ones_like(interaction_df, dtype=bool))\n",
    "    #custom palette\n",
    "    #cmap = sns.color_palette(\"rocket_r\", as_cmap=True)\n",
    "    cmap = sn.color_palette(\"Reds\", 5)\n",
    "    #cmap = sns.color_palette(\"WtRd\", 5)\n",
    "    cmap = sn.color_palette(\"RdBu_r\", 5)\n",
    "    #cmap = sns.color_palette(\"vlag\", as_cmap=True)\n",
    "    #cmap= sns.cubehelix_palette(start=5, rot=0, dark=0.25, light=1.5)\n",
    "    my_center = 0.5*(interaction_df.to_numpy().mean(0).max()-interaction_df.to_numpy().mean(0).min())+interaction_df.to_numpy().mean(0).min()\n",
    "    my_center=0.00\n",
    "    ax = plt.axes()\n",
    "    \n",
    "    \n",
    "    sn.heatmap(interaction_df, center = my_center,\n",
    "                square=True, mask=mask, cmap=cmap)\n",
    "    ax.set_title(target_label)\n",
    "    #interaction_df.style.background_gradient(cmap='Blues')\n",
    "    out_prefix = 'SHAP_feature_interaction_importance_'+target_label+'_'+model_name.replace(' ','_')\n",
    "    plt.savefig(out_prefix+\".pdf\", bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    df = interaction_df.where(np.triu(np.ones(interaction_df.shape)).astype(bool))\n",
    "    df = df.stack().reset_index()\n",
    "    df.columns = ['source','target','weight']\n",
    "    df['color']=df['weight']\n",
    "    df.color[df.weight < 0] = \"blue\"\n",
    "    df.color[df.weight > 0] = \"red\"\n",
    "    df.color[df.weight == 0] = \"black\"\n",
    "    # Keep only correlation over a threshold and remove self correlation (cor(A,A)=1)\n",
    "    Q1=np.quantile(interaction_df.to_numpy(), .20)\n",
    "    Q3=np.quantile(interaction_df.to_numpy(), .80)\n",
    "    print(Q1,Q3)\n",
    "    out_prefix='SHAP_edges_filtered_extreme_quartiles_'+target_label+'_'+model_name.replace(' ','_')\n",
    "    edges_filtered=df.loc[ ((df['weight'] >= Q3) | (df['weight'] <= Q1)) & (df['source'] != df['target']) ]\n",
    "    plt.figure(3,figsize=(12,12)) \n",
    "    edges_filtered.to_csv(out_prefix+'.csv', index=False)\n",
    "    \n",
    "    out_prefix='SHAP_edges_unfiltered_'+target_label+'_'+model_name.replace(' ','_')\n",
    "    \n",
    "    df.to_csv(out_prefix+'.csv', index=False)\n",
    "\n",
    "    print(\"head\",edges_filtered.head())\n",
    "    #scaler = preprocessing.MinMaxScaler(feature_range=(-1,1)).fit(np.array(edges_filtered['weight']).reshape(-1, 1))\n",
    "    #edges_filtered['weight'] = scaler.transform(np.array(edges_filtered['weight']).reshape(-1, 1))\n",
    "\n",
    "\n",
    "    # Keep only correlation over a threshold and remove self correlation (cor(A,A)=1)\n",
    "    threshold = cor_threshold\n",
    "    #links_filtered=links.loc[ ((links['value'] > threshold) | (links['value'] < -threshold)) & (links['var1'] != links['var2']) ]\n",
    "\n",
    "    # create a new graph from edge list\n",
    "    Gx = nx.from_pandas_edgelist(edges_filtered, \"source\", \"target\", edge_attr=[\"weight\"])\n",
    "    \n",
    "\n",
    "    # list to store edges to remove\n",
    "    remove = []\n",
    "    keep = []\n",
    "    # loop through edges in Gx and find correlations which are below the threshold\n",
    "    for var1, var2 in Gx.edges():\n",
    "        corr = Gx[var1][var2][\"weight\"]\n",
    "        # add to remove node list if abs(corr) < threshold\n",
    "        if abs(corr) < threshold:\n",
    "            remove.append((var1, var2))\n",
    "        elif var1 == var2:\n",
    "            remove.append((var1, var2))\n",
    "        else:\n",
    "            keep.append((var1, var2))\n",
    "\n",
    "    # remove edges contained in the remove list\n",
    "    Gx.remove_edges_from(remove)\n",
    "    Gx.remove_nodes_from(list(nx.isolates(Gx)))\n",
    "\n",
    "    print(str(len(remove)) + \" edges removed, remaining: \" + str(len(keep)))\n",
    "\n",
    "    \n",
    "    \n",
    "   \n",
    "    Gx.remove_nodes_from(list(nx.isolates(Gx)))\n",
    "    print(edges_filtered.describe())\n",
    "    # Build your graph\n",
    "    #G=nx.from_pandas_edgelist(links_filtered, 'var1', 'var2')\n",
    "    #G.edges(data=True)\n",
    "\n",
    "    edge_colours = []\n",
    "    edge_width = []\n",
    "    for key, value in nx.get_edge_attributes(Gx, \"weight\").items():\n",
    "        edge_colours.append(assign_colour(value))\n",
    "        edge_width.append(assign_thickness(value, benchmark_thickness=50, scaling_factor=1))\n",
    "\n",
    "    # assign node size depending on number of connections (degree)\n",
    "    node_size = []\n",
    "    for key, value in dict(Gx.degree).items():\n",
    "        node_size.append(assign_node_size(value, scaling_factor=300))\n",
    "\n",
    "    df = pd.DataFrame(index=Gx.nodes(), columns=Gx.nodes())\n",
    "    for row, data in nx.shortest_path_length(Gx):\n",
    "        for col, dist in data.items():\n",
    "            df.loc[row,col] = dist\n",
    "\n",
    "    df = df.fillna(df.max().max())\n",
    "    df = df+(df.max().max())\n",
    "    layout = nx.kamada_kawai_layout(Gx, dist=df.to_dict())\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    #plt.figure(figsize=(7,6))\n",
    "\n",
    "\n",
    "    #sn.set(rc={\"figure.figsize\": (10, 10)})\n",
    "    plt.figure(figsize=(7,6))\n",
    "\n",
    "    nx.draw(\n",
    "        Gx,\n",
    "        #pos=nx.fruchterman_reingold_layout(Gx),\n",
    "        #pos=nx.spectral_layout(Gx),\n",
    "        #pos=nx.circular_layout(Gx),\n",
    "        #pos=nx.nx_agraph.graphviz_layout(Gx, prog=\"neato\"),\n",
    "        #pos=nx.spring_layout(Gx),\n",
    "        #pos=nx.nx_pydot.graphviz_layout(Gx),\n",
    "        pos=layout,\n",
    "        with_labels=True,\n",
    "        node_size=node_size,\n",
    "        node_color=assign_node_colors(Gx),#\"#e1575c\",\n",
    "        edge_color=edge_colours,\n",
    "        width=edge_width,\n",
    "        #width=1.2,\n",
    "        font_size=8,\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    out_prefix='SHAP_interaction_network_'+target_label+'_'+model_name.replace(' ','_')\n",
    "    \n",
    "    plt.savefig(out_prefix+'.png',dpi=400, bbox_inches='tight')\n",
    " \n",
    "    #ax.legend(*get_legend(np.array(node_size), num=4), title=\"Degree\")\n",
    "    \n",
    "    #plt.colorbar(ec)\n",
    "    #plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    return interaction_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
